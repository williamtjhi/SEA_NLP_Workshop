{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f97e83f",
   "metadata": {},
   "source": [
    "## A more accurate Indonesian NER using IndoBERT - a monolingual model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c65d2e",
   "metadata": {},
   "source": [
    "IndoNLU is a recent development done by a collaborative effort between Gojek, ITB, Airy and others (https://github.com/indobenchmark/indonlu). It builds the Indo4B dataset for various Indonesian native NLP tasks including NER, sentiment analysis etc. It also uses Indo4B to develop and Indonesian version of BERT called IndoNLU's IndoBERT. \n",
    "\n",
    "We use IndoBERT here to demonstrate the performance of NER based on a monolingual model. We have fine-tuned the base IndoBERT for NER using the NER-Grit dataset. We used the fine-tuning script provided by IndoNLU here: https://github.com/indobenchmark/indonlu/blob/master/examples/finetune_ner_grit.ipynb, saved the fine-tuned model locally in the `model` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638c184e",
   "metadata": {},
   "source": [
    "**Notes:** this demo requires `transformers` version `2.9.0`. For this reason, to run this demo, please first exit from your current environment, create a new environment and execute `pip install -r requirements_IndoBERT.txt` before running this notebook. In addition, we need to copy to our current directory the `word_classification.py` from the IndoNLU repo: https://github.com/indobenchmark/indonlu/blob/master/modules/word_classification.py. If you clone the SEA_NLP_workshop repository, this copying has already been performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57655f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer #need transformers==2.9.0\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from word_classification import BertForWordClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc1175",
   "metadata": {},
   "source": [
    "Loading IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7243ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6b4e2",
   "metadata": {},
   "source": [
    "Mapping the named entity labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a0d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_labels = 7\n",
    "i2w = {0: 'I-PERSON', 1: 'B-ORGANISATION', 2: 'I-ORGANISATION', 3: 'B-PLACE', 4: 'I-PLACE', 5: 'O', 6: 'B-PERSON'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa6161",
   "metadata": {},
   "source": [
    "Loading the fine-tuned model that has been saved locally before. It is possible to do your own fine-tuning by executing the notebook https://github.com/indobenchmark/indonlu/blob/master/examples/finetune_ner_grit.ipynb yourself. Do note that it requires `transformers` version `2.9.0` for it to work.\n",
    "\n",
    "We use `cpu` as our device as we are performing inference only here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d4e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForWordClassification.from_pretrained('models/', config=config).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b262397",
   "metadata": {},
   "source": [
    "Defining a function to convert a text into IndoBERT tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b55218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_subword_tokenize(sentence, tokenizer):\n",
    "    # Add CLS token\n",
    "    subwords = [tokenizer.cls_token_id]\n",
    "    subword_to_word_indices = [-1] # For CLS\n",
    "\n",
    "    # Add subwords\n",
    "    for word_idx, word in enumerate(sentence):\n",
    "        subword_list = tokenizer.encode(word, add_special_tokens=False)\n",
    "        subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
    "        subwords += subword_list\n",
    "\n",
    "    # Add last SEP token\n",
    "    subwords += [tokenizer.sep_token_id]\n",
    "    subword_to_word_indices += [-1]\n",
    "\n",
    "    return subwords, subword_to_word_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c45dd6",
   "metadata": {},
   "source": [
    "Perform some tests on some Indonesian sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d3e7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nama</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saya</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Budi</td>\n",
       "      <td>B-PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Saya</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lahir</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>di</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tahun</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1981</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sekarang</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>saya</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tinggal</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>di</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ubud</td>\n",
       "      <td>B-PLACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bali</td>\n",
       "      <td>B-PLACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       words     label\n",
       "0       Nama         O\n",
       "1       saya         O\n",
       "2       Budi  B-PERSON\n",
       "3          .         O\n",
       "4       Saya         O\n",
       "5      lahir         O\n",
       "6         di         O\n",
       "7      tahun         O\n",
       "8       1981         O\n",
       "9          .         O\n",
       "10  Sekarang         O\n",
       "11      saya         O\n",
       "12   tinggal         O\n",
       "13        di         O\n",
       "14      Ubud   B-PLACE\n",
       "15         ,         O\n",
       "16      Bali   B-PLACE\n",
       "17         .         O"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize('Nama saya Budi. Saya lahir di tahun 1981. Sekarang saya tinggal di Ubud, Bali.')\n",
    "#My name is Budi. I was born in 1981. I am currently living in Ubud, Bali.\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n",
    "\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].squeeze().cpu().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "\n",
    "pd.DataFrame({'words': text, 'label': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c6d9b",
   "metadata": {},
   "source": [
    "Contrast the following test result against the output of the multilingual NER based on XLM-Roberta (see the notebook `Multilingual_NER`). Here with IndoBERT, *menara bca* is correctly understood from context to be a location. With XLM-ROBERTA, it failed to recognise the context and tagged *menara* as LOCATION and *bca* as organisation independently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc41107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Budi</td>\n",
       "      <td>B-PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sudah</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sampai</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>di</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>depan</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>menara</td>\n",
       "      <td>B-PLACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bca</td>\n",
       "      <td>I-PLACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words     label\n",
       "0    Budi  B-PERSON\n",
       "1   sudah         O\n",
       "2  sampai         O\n",
       "3      di         O\n",
       "4   depan         O\n",
       "5  menara   B-PLACE\n",
       "6     bca   I-PLACE"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize('Budi sudah sampai di depan menara bca')\n",
    "#Budi has arrived in front of the BCA Tower.\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n",
    "\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].squeeze().cpu().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "\n",
    "pd.DataFrame({'words': text, 'label': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f516e1b4",
   "metadata": {},
   "source": [
    "Similarly, here IndoBERT recognises the context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d63103ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yani</td>\n",
       "      <td>B-PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pergi</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ke</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mall</td>\n",
       "      <td>B-PLACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>taman</td>\n",
       "      <td>I-PLACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anggrek</td>\n",
       "      <td>I-PLACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>membeli</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kue</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bantal</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     words     label\n",
       "0     Yani  B-PERSON\n",
       "1    pergi         O\n",
       "2       ke         O\n",
       "3     mall   B-PLACE\n",
       "4    taman   I-PLACE\n",
       "5  anggrek   I-PLACE\n",
       "6  membeli         O\n",
       "7      kue         O\n",
       "8   bantal         O"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize('Yani pergi ke mall taman anggrek membeli kue bantal')\n",
    "#Yani went to Taman Anggrek Mall to buy a Pillow Cake.\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n",
    "\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].squeeze().cpu().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "\n",
    "pd.DataFrame({'words': text, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d57ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
